<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>KL-Divergence</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <nav class="navbar">
        <div class="navbar-container">
            <a class="navbar-logo" href="index.html">Prashant Gavit</a>
            <div class="navbar-links">
                <a href="../index.html#about">About</a>
                <a href="../index.html#experience">Experience</a>
                <a href="mailto:prashat.gavit@sjsu.edu">Contact</a>
                <a href="https://prashantgavit.github.io/prashant_gavit_mle.pdf" target="_blank">Resume</a>
                <a href="blogs.html">Blogs</a>
            </div>
        </div>
    </nav>
    <section class="blog-post">
        <h1 class="blog-post-title">KL Divergence: From Intuition to Formula</h1>
        <h2 class="blog-post-subtitle">Why I am writing about KL Divergence?</h2>
        <p class="blog-post-content">
            I was working on a variational autoencoder, which uses KL Divergence as one of the loss functions to optimize the model.
            The formula for KL divergence seemed intimidating at first, so I wanted to understand it in detail.
            I found a very interesting <a href="https://www.youtube.com/watch?v=q0AkK8aYbLY" target="_blank" class="blog-post-link">YouTube video by Ritvik Math</a>,
            and decided to replicate the explanation here for my future reference and for anyone who prefers reading over watching an 18-minute video.
        </p>
        <h2 class="blog-post-subtitle">Why do we need KL divergence?</h2>
        
        <p class="blog-post-content">
            KL divergence, or Kullback-Leibler divergence, measures how one probability 
            distribution differs from another reference distribution.
            Sometimes, we want to compare two different probability distributions to understand how similar or different they are.  
            This kind of comparison is useful in many areas, such as statistics, machine learning, and data science. 
            For example, we might want to know how the population height distribution in one country compares to that in another—this could reveal differences in nutrition, healthcare, or genetics.  

        </p>

        <p class="blog-post-content">
            Now, how do we define this metric? Let's start by creating our own metric to quantify the difference, 
            then discuss its limitations, and finally move towards the actual KL divergence metric by addressing 
            those limitations.
        </p>

        <h2 class="blog-post-subtitle">What Should Our Metric Capture?</h2>
        <div style="display: flex; gap: 40px; justify-content: center; margin: 32px 0;">
            <div>
            <h3 style="text-align:center;">Batch of 2024 (P)</h3>
            <canvas id="chart1" width="300" height="250"></canvas>
            </div>
            <div>
            <h3 style="text-align:center;">Batch of 2025 (Q)</h3>
            <canvas id="chart2" width="300" height="250"></canvas>
            </div>
        </div>

        <div style="max-width: 700px; margin: 0 auto 24px auto; text-align: center; ; color: #444;">
            <p style="font-size: 0.85em">
                The graphs above illustrate the percentage distribution of students’ fruit preferences for the 2024 and 2025 batches.
                We’ll refer to the 2024 distribution as P distribution and the 2025 distribution as Q distribution.
                Now, you might be thinking—a student could like more than one fruit, or none at all—so shouldn’t the percentages sometimes exceed or fall short of 100%?
                To keep things simple, let's assume we live in a world where each student likes exactly one type of fruit—no more, no less.
                Please don’t overthink it.
            </p>
        </div>
        <div style="max-width: 700px; margin: 0 auto 24px auto; text-align: center; color: #444;">
            <p style="font-size: 1em; font-weight: bold;">
            Figure 1
            </p>
        </div>

        <p class="blog-post-content">
           Our goal is to define a metric that tells us how similar or different the two distributions shown in Figure 1 are.
           Before we get into the details, let’s outline the key attributes this metric should have:
        </p>

        <p class="blog-post-content">

        <strong>Quantification:</strong> It should provide a clear and meaningful measure of the similarity or difference between the two distributions.
        
        </p>
        
        <p class="blog-post-content">

        <strong>Asymmetry:</strong> The metric should reflect that the difference from the second distribution to the first may not be the same as the reverse. Since we are using the first distribution as a benchmark, the metric should be able to capture directionality—indicating not just how much the distributions differ, but also in which direction. This could be reflected in the sign or scale of the value.
        
        </p>


        

            <h2 class="blog-post-subtitle">A Simple Approach: The Layman’s Metric</h2>
            <p class="blog-post-content">
                Let’s examine how the values scale for each fruit relative to one another, 
                and then compute the average of those scaling values.
                In other words, we want to understand how distribution P changes with respect to distribution Q.
                <span style="display: block; text-align: center; margin: 24px 0;">
                $$
                \text{Divergence} = \frac{1}{3} \left( \frac{P(\text{Orange})}{Q(\text{Orange})} + \frac{P(\text{Mango})}{Q(\text{Mango})} + \frac{P(\text{Apple})}{Q(\text{Apple})} \right) \tag{1}
                $$
                </span>
                <span style="display: block; text-align: center; margin: 24px 0;">
                $$
                \text{Divergence} = \frac{1}{3} \left( \frac{{50}}{{50}} + \frac{10}{40} + \frac{40}{10} \right) 
                $$
                </span>

                <span style="display: block; text-align: center; margin: 24px 0;">
                $$
                \text{Divergence} = 1.75 
                $$
                </span>
                
                <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
                <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            </p>

        <p class="blog-post-content">
            As shown in Equation 1, the divergence value quantifies the difference between the two distributions and also
            captures the asymmetry in the comparison. For example, if we are measuring the divergence of P with respect to Q, 
            the ratio is  \( \frac{P}{Q} \);  whereas if we compare Q with respect to P, the ratio becomes \( \frac{Q}{P} \).
        </p>

        <h2 class="blog-post-subtitle">Where the Simple Metric Falls Short—and How to Fix It</h2>

        <!-- <p class="blog-post-content">
            As we take average over each descrept category (e.g. Mango, Orange and Apple), the \( \frac{P(\text{Apple})}{Q(\text{Apple})} \) 
            value which 4 is very high compare \( \frac{P(\text{Mango})}{Q(\text{Mango})} \) which is \( \frac{1}{4} \). So 4 can skew the importance of \( \frac{1}{4} \). 
            How can we solve that? Can we define a function \( f(x) = y \) such that if we put \( f(4) \) it returns \( k \), 
            and \(f( \frac{1}{4} \)) returns \( -k \). And fotunately we already have such function \( f(x) = log(x) \). so let change our equation 1 to equation 2.

        </p> -->
        <p class="blog-post-content">
        As we take the average over each discrete category (e.g., Mango, Orange, and Apple), we observe that the value 
        \( \frac{P(\text{Apple})}{Q(\text{Apple})} = 4 \) is significantly higher compared to 
        \( \frac{P(\text{Mango})}{Q(\text{Mango})} = \frac{1}{4} \). This large value of 4 can disproportionately skew the importance 
        of the smaller value \( \frac{1}{4} \).
        </p>

        <p class="blog-post-content">
        How can we address this imbalance? One solution is to define a transformation function \( f(x) = y \) such that 
        \( f(4) = k \) and \( f\left(\frac{1}{4}\right) = -k \). Fortunately, we already have such a function: 
        \( f(x) = \log(x) \).
        </p>

        <p class="blog-post-content">
        Therefore, let’s update Equation 1 to reflect this transformation and define it as Equation 2.
        </p>

        <p>
                <span style="display: block; text-align: center; margin: 24px 0;">
                $$
                \text{Divergence} = \frac{1}{3} \sum_{x \in \{\text{Orange},\,\text{Mango},\,\text{Apple}\}} \log \frac{P(x)}{Q(x)} \tag{2}
                $$
                </span>
        </p>

    <p class="blog-post-content">
        Equation 2 helps address the issue of skewness by applying the logarithm, but can we go further to better handle edge cases related to asymmetry?
        For instance, although asymmetry is introduced at the category level, the overall divergence remains the same when summed across all categories—
        regardless of whether we compute it with respect to <strong>P</strong> or <strong>Q</strong>.
    </p>

    <p class="blog-post-content">
        To tackle this, instead of assigning equal weight (\( \frac{1}{n} \)) to each term like 
        \( \log \frac{P(\text{Mango})}{Q(\text{Mango})} \), we should weigh each term according to the importance of that category.
        This importance is determined by the value of the reference distribution for the given category.
    </p>

    <p class="blog-post-content">
        With this adjustment, Equation 2 can be modified to form Equation 3.
    </p>

    <p class="blog-post-content">
        <span style="display: block; text-align: center; margin: 24px 0;">
            $$
            \text{Divergence} = \sum_{x in \{\text{Orange},\,\text{Mango},\,\text{Apple}\}} {P(x)} \log \frac{P(x)}{Q(x)} \tag{3}
            $$
        </span>
    </p>

    <h2 class="blog-post-subtitle">The Big Reveal: KL Divergence Unveiled</h2>

    <p class="blog-post-content">
        Equation 3 is, in fact, the KL divergence—though it doesn't appear intimidating, right?  
        That's because we derived it using a discrete distribution, which gives us a simpler summation form.  
        However, if we replace the summation with an integral, we arrive at the exact KL divergence formula, as shown in Equation 4.
        <span style="display: block; text-align: center; margin: 24px 0;">
            $$
            D_{KL}(P \parallel Q) = \int_{-\infty}^{\infty} P(x) \log \frac{P(x)}{Q(x)} \, dx \tag{4}
            $$
        </span>
    </p>

 

        <div style="text-align:left;margin-top:32px;">
            <a href="blogs.html" class="blog-post-link">&#8592; Back to Blogs</a>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        <script>
        const labels = ['Orange', 'Mango', 'Apple'];
        const data1 = [50, 10, 40];
        const data2 = [50, 40, 10];

        const barColor = '#d97e7b'; // Red

        const chartOptions = {
            animation: false,
            plugins: {
            legend: { display: false }
            },
            scales: {
            x: {
            grid: { display: false },
            ticks: { color: '#333' },
            categoryPercentage: 0.5,
            barPercentage: 0.5,
            },
            y: {
            grid: { display: false },
            beginAtZero: true,
            max: 60,
            ticks: { color: '#333' }
            }
            }
        };

        new Chart(document.getElementById('chart1').getContext('2d'), {
            type: 'bar',
            data: {
            labels: labels,
            datasets: [{
            label: '% of Students',
            data: data1,
            backgroundColor: Array(labels.length).fill(barColor),
            borderRadius: 6,
            }]
            },
            options: chartOptions
        });

        new Chart(document.getElementById('chart2').getContext('2d'), {
            type: 'bar',
            data: {
            labels: labels,
            datasets: [{
            label: '% of Students',
            data: data2,
            backgroundColor: Array(labels.length).fill(barColor),
            borderRadius: 6,
            }]
            },
            options: chartOptions
        });
        </script>

    </section>
</body>
</html>
